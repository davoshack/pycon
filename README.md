# ğŸ“˜ Laboratorio Local de IA con Docker Model Runner

## âœ¨ Objetivo del taller
Este laboratorio te guÃ­a paso a paso para ejecutar un modelo de lenguaje (LLM) localmente usando Docker Model Runner, con dos interfaces posibles:

- **Open WebUI**: Una interfaz web estilo ChatGPT.
- **Python (Streamlit y consola)**: Para quienes desean personalizar su experiencia o integrar con sus propios proyectos.

---

## ğŸ§° Requisitos
- âœ… Docker Desktop v4.40 o superior
- âœ… Terminal (macOS, Linux, Git Bash o PowerShell)
- âœ… ConexiÃ³n inicial para descargar el modelo `.gguf`
- âœ… Python 3.9+ con `llama-cpp-python` y `streamlit` para uso personalizado

---

## ğŸ—‚ï¸ Estructura del proyecto

```
llama_lab/
â”œâ”€â”€ app.py                # Chatbot en Streamlit
â”œâ”€â”€ main.py               # Interfaz por consola con selecciÃ³n de modelo
â”œâ”€â”€ requirements.txt      # Dependencias Python (llama-cpp-python, streamlit)
â”œâ”€â”€ README.md             # Esta guÃ­a
â”œâ”€â”€ LICENSE
â”œâ”€â”€ .gitignore
â””â”€â”€ models/               # Carpeta donde se guarda el modelo GGUF
```

---

## ğŸ§ª Parte 1: Laboratorio Web con Docker y Open WebUI

### ğŸ”¹ Paso 1: Activar Docker Model Runner
```bash
docker desktop enable model-runner --no-tcp
```

### ğŸ”¹ Paso 2: Descargar un modelo ligero
```bash
docker model pull ai/gemma3-qat:1B-Q4_K_M
```

### ğŸ”¹ Paso 3: Crear el archivo `docker-compose.yaml`
```yaml
services:
  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    ports:
      - "3000:8080"
    environment:
      - OPENAI_API_BASE_URL=http://model-runner.docker.internal:80/engines/llama.cpp/v1
      - OPENAI_API_KEY=na
    volumes:
      - open-webui-data:/app/backend/data
    depends_on:
      - ai-runner

  ai-runner:
    provider:
      type: model
      options:
        model: ai/gemma3-qat:1B-Q4_K_M

volumes:
  open-webui-data:
```

### ğŸ”¹ Paso 4: Ejecutar el entorno
```bash
docker compose up
```

### ğŸ”¹ Paso 5: Abrir la interfaz en tu navegador
[http://localhost:3000](http://localhost:3000)

---

## ğŸ Parte 2: Laboratorio en Python (Opcional)

### 1ï¸âƒ£ Ejecutar en consola
```bash
python main.py
```
Selecciona un modelo `.gguf` y comienza a escribir tus prompts.

### 2ï¸âƒ£ Ejecutar interfaz grÃ¡fica con Streamlit
```bash
streamlit run app.py
```

InteractÃºa con el modelo local usando una interfaz estilo chat. Ideal para demos o personalizaciÃ³n.

---

## ğŸ“¦ Requisitos de Python

Instala dependencias con:

```bash
pip install -r requirements.txt
```

Contenido de `requirements.txt`:
```
llama-cpp-python==0.3.10
streamlit
```

---

## ğŸ§¼ Para detener Docker
```bash
Ctrl + C
docker compose down
```

---

## ğŸ’« CrÃ©ditos y agradecimientos

Gracias por ser parte de este taller de IA local ğŸ’»ğŸ³  
Â¡Ahora tienes un laboratorio mÃ¡gico de LLMs corriendo directamente en tu mÃ¡quina! âœ¨
