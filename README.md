# Taller: Laboratorio local de IA

**TÃ­tulo:** Laboratorio local de IA: Corre modelos con Python y Docker sin complicaciones  
**DuraciÃ³n estimada:** 2 a 2.5 horas  
**Objetivo:** Aprender a correr modelos de IA de manera local usando Docker Model Runner sin necesidad de configurar entornos complejos. DescubrirÃ¡s cÃ³mo usar Docker para levantar un modelo de lenguaje en tu mÃ¡quina, consultar su API y experimentar con IA de forma accesible y reproducible.

---

## ğŸ”§ Requisitos previos

Antes de comenzar, asegÃºrate de tener instalado:

- **[Docker Desktop](https://www.docker.com/products/docker-desktop/)**: esta es la herramienta principal. Incluye el motor de Docker que nos permitirÃ¡ ejecutar contenedores.
- **Git**: Ãºtil para clonar repositorios con ejemplos.
- **Editor de texto**: Visual Studio Code o cualquier editor que te guste.

AdemÃ¡s:
- Tener espacio libre en disco (~5GB) ya que algunos modelos ocupan bastante.
- ConexiÃ³n a internet estable para descargar las imÃ¡genes y modelos.
- Familiaridad bÃ¡sica con la terminal o consola de comandos.

---

## 1. ğŸŒŸ IntroducciÃ³n al taller

### Â¿QuÃ© haremos hoy?
- UsarÃ¡s Docker para correr un modelo de lenguaje natural localmente.
- InteractuarÃ¡s con ese modelo desde tu mÃ¡quina, sin necesidad de instalar librerÃ­as como Transformers o TensorFlow.
- EntenderÃ¡s cÃ³mo funciona el archivo `model-runner.yaml` y cÃ³mo cambiar el modelo fÃ¡cilmente.

### Â¿Por quÃ© usar Docker?
- Imagina que el modelo es como una app complicada de instalar. Docker te da una "cajita" lista para usar.
- Todo lo necesario (sistema, librerÃ­as, entorno) estÃ¡ empaquetado en un contenedor.
- Puedes compartir o mover esa "cajita" a cualquier mÃ¡quina y funcionarÃ¡ igual.
- Te ahorra tiempo, evita errores de configuraciÃ³n y mantiene tu entorno limpio.

---

## 2. ğŸ¤– Â¿QuÃ© es Docker Model Runner?

Docker Model Runner es una soluciÃ³n pensada para simplificar el uso de modelos de IA en contenedores. EstÃ¡ desarrollado por Docker y hace que correr modelos como LLaMA, Mistral, Gemma u otros, sea tan fÃ¡cil como definir dos lÃ­neas en un archivo de configuraciÃ³n.

### Â¿CÃ³mo funciona?
- Lee el archivo `model-runner.yaml`.
- SegÃºn el modelo y backend definido (como `ollama`), descarga y configura automÃ¡ticamente el contenedor correcto.
- Te expone una API tipo OpenAI que puedes consultar desde Python, cURL o tu navegador.

### Â¿QuÃ© es Ollama?
- Ollama es una tecnologÃ­a que permite correr modelos LLM localmente, optimizados y fÃ¡cilmente integrables.
- Docker Model Runner lo usa como backend por defecto para muchos modelos.

---

## 3. ğŸ“ Clonar el repositorio de ejemplo

Usaremos un ejemplo compartido por Bret Fisher. Para empezar, abre la terminal y ejecuta:

```bash
git clone https://gist.github.com/BretFisher/aafd46eeb7acef2f5ef7d1ea70abe2ad model-runner-demo
cd model-runner-demo
```

Esto descargarÃ¡ el archivo `model-runner.yaml` que usaremos.

---

## 4. ğŸ“ Explorar el archivo `model-runner.yaml`

Abre el archivo con tu editor:

```bash
code model-runner.yaml
```

Contenido tÃ­pico:

```yaml
model: llama3
backend: ollama
```

- `model`: el nombre del modelo que quieres correr. En este caso, es `llama3`.
- `backend`: define cÃ³mo se corre el modelo. AquÃ­ usamos `ollama`, que permite modelos LLM locales.

Este archivo actÃºa como la receta que Docker Model Runner va a leer.

---

## 5. ğŸ³ Ejecutar el modelo con Docker CLI

Ahora vamos a correr el modelo localmente con Docker:

```bash
docker run -it --rm \
  -v $(pwd)/model-runner.yaml:/etc/model-runner/model-runner.yaml \
  -p 11434:11434 \
  docker/model-runner
```

### Â¿QuÃ© hace cada parte?
- `docker run`: comando para iniciar un contenedor.
- `-it`: modo interactivo.
- `--rm`: borra el contenedor al salir.
- `-v`: monta tu archivo `model-runner.yaml` dentro del contenedor.
- `-p 11434:11434`: abre el puerto 11434 (el modelo corre ahÃ­).
- `docker/model-runner`: imagen oficial que correrÃ¡ el modelo por ti.

Cuando el modelo estÃ© listo verÃ¡s algo como:

```
Model loaded. Listening on :11434
```

Eso significa que ya puedes consultarlo desde tu mÃ¡quina.

---

## 6. ğŸ§ª Probar el modelo con cURL

Abre otra terminal (sin cerrar la que tiene Docker corriendo) y ejecuta:

```bash
curl http://localhost:11434/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "llama3",
    "messages": [{"role": "user", "content": "Â¿QuÃ© es la inteligencia artificial?"}]
}'
```

### Â¿QuÃ© hace esto?
- Le envÃ­a un mensaje al modelo usando la API.
- El modelo responde con una respuesta generada.

VerÃ¡s una respuesta en formato JSON con el contenido del modelo, similar al estilo de OpenAI.

---

## 7. ğŸ” Cambiar de modelo

Si quieres probar otro modelo:

1. Edita `model-runner.yaml`:

```yaml
model: mistral
backend: ollama
```

2. Vuelve a correr el comando de `docker run`.

**Consejo:** consulta modelos compatibles aquÃ­ ğŸ‘‰ [https://ollama.com/library](https://ollama.com/library)

---

## 8. ğŸ Usar Python para interactuar con el modelo

Si prefieres escribir un script en Python:

### Crea `chat.py` con este contenido:

```python
import requests

url = "http://localhost:11434/v1/chat/completions"
headers = {"Content-Type": "application/json"}
payload = {
    "model": "llama3",
    "messages": [{"role": "user", "content": "Hola, Â¿cÃ³mo estÃ¡s?"}]
}

response = requests.post(url, headers=headers, json=payload)
print(response.json())
```

### Ejecutar:

```bash
python chat.py
```

Esto hace exactamente lo mismo que el `curl`, pero en tu propio cÃ³digo. Â¡Listo para usar en apps reales!

---

## 9. âœ… Cierre del taller

### Â¿QuÃ© aprendimos?
- CÃ³mo levantar un modelo de lenguaje local usando solo Docker.
- CÃ³mo consultar su API local con `curl` y Python.
- CÃ³mo cambiar el modelo editando un simple archivo YAML.

### Â¿QuÃ© podrÃ­as hacer ahora?
- Crear una pequeÃ±a app con FastAPI y conectarla al modelo.
- Usar modelos que respondan en espaÃ±ol.
- Hacer un chatbot local sin conexiÃ³n a internet usando tu propio modelo.

### Recursos Ãºtiles
- [Docker Model Runner Docs](https://docs.docker.com/ai/model-runner/)
- [Ollama Library](https://ollama.com/library)
- [Gist de Bret Fisher](https://gist.github.com/BretFisher/aafd46eeb7acef2f5ef7d1ea70abe2ad)

---

Gracias por ser parte de este taller ğŸ’« Â¡Ahora tienes un laboratorio de IA corriendo en tu mÃ¡quina! ğŸ§ªğŸ³
